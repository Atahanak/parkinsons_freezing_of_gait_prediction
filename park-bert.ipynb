{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchmetrics.functional.classification import multiclass_average_precision\n",
    "import torchmetrics\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Author: Taha Atahan Akyildiz\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.9\n",
      "IPython version      : 8.10.0\n",
      "\n",
      "numpy            : 1.23.5\n",
      "pandas           : 1.5.3\n",
      "torch            : 2.0.0\n",
      "pytorch_lightning: 2.0.0\n",
      "sklearn          : 1.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Taha Atahan Akyildiz\" -d -t -v -p numpy,pandas,torch,pytorch_lightning,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': '__main__', 'KAGGLE': False, 'ROOT_READ': '../', 'ROOT_WRITE': '../', 'DATA_DIR': '../tlvmc-parkinsons-freezing-gait-prediction/', 'TRAIN_DIR': '../tlvmc-parkinsons-freezing-gait-prediction/train/', 'TDCSFOG_DIR': '../tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/', 'DEFOG_DIR': '../tlvmc-parkinsons-freezing-gait-prediction/train/defog/', 'CHECKPOINT_PATH': '../checkpoints/', 'PARAMS_PATH': './config.json', 'f': <_io.TextIOWrapper name='./config.json' mode='r' encoding='UTF-8'>, 'hparams': {'batch_size': 1024, 'window_size': 32, 'window_future': 8, 'wx': 1, 'model_dropout': 0.2, 'model_hidden': 512, 'model_nblocks': 1, 'model_nhead': 8, 'lr': 0.00015, 'milestones': [5, 10, 15, 20], 'gamma': 3e-05, 'num_epochs': 1, 'use_pretrained': False}, 'batch_size': 1024, 'window_size': 32, 'window_future': 8, 'window_past': 24, 'wx': 1, 'model_dropout': 0.2, 'model_hidden': 512, 'model_nblocks': 1, 'lr': 0.00015, 'milestones': [5, 10, 15, 20], 'gamma': 3e-05, 'num_epochs': 1, 'device': 'cpu', 'num_workers': 1, 'feature_list': ['AccV', 'AccML', 'AccAP'], 'label_list': ['StartHesitation', 'Turn', 'Walking'], '__dict__': <attribute '__dict__' of 'Config' objects>, '__weakref__': <attribute '__weakref__' of 'Config' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    KAGGLE = False\n",
    "    ROOT_READ = '../'\n",
    "    ROOT_WRITE = '../'\n",
    "    if KAGGLE:\n",
    "        ROOT_READ = '/kaggle/input/'\n",
    "        ROOT_WRITE = '/kaggle/working/'\n",
    "    DATA_DIR = f'{ROOT_READ}tlvmc-parkinsons-freezing-gait-prediction/'\n",
    "    TRAIN_DIR = f'{ROOT_READ}tlvmc-parkinsons-freezing-gait-prediction/train/'\n",
    "    TDCSFOG_DIR = f'{ROOT_READ}tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/'\n",
    "    DEFOG_DIR = f'{ROOT_READ}tlvmc-parkinsons-freezing-gait-prediction/train/defog/'\n",
    "    CHECKPOINT_PATH = f'{ROOT_WRITE}checkpoints/'\n",
    "    PARAMS_PATH = f'./config.json'\n",
    "    if KAGGLE:\n",
    "        PARAMS_PATH = '/' # TODO\n",
    "\n",
    "\n",
    "    with open(PARAMS_PATH) as f:\n",
    "        hparams = json.load(f)\n",
    "\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    window_size = hparams[\"window_size\"]\n",
    "    window_future = hparams[\"window_future\"]\n",
    "    window_past = window_size - window_future\n",
    "    wx = hparams[\"wx\"]\n",
    "\n",
    "    model_dropout = hparams[\"model_dropout\"]\n",
    "    model_hidden = hparams[\"model_hidden\"]\n",
    "    model_nblocks = hparams[\"model_nblocks\"]\n",
    "\n",
    "    lr = hparams[\"lr\"]\n",
    "    milestones = hparams[\"milestones\"]\n",
    "    gamma = hparams[\"gamma\"]\n",
    "\n",
    "    num_epochs = hparams[\"num_epochs\"]\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    num_workers = 32 if torch.cuda.is_available() else 1\n",
    "\n",
    "    feature_list = ['AccV', 'AccML', 'AccAP']\n",
    "    label_list = ['StartHesitation', 'Turn', 'Walking']\n",
    "\n",
    "cfg = Config()\n",
    "print(vars(Config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class FOGDataset(Dataset):\n",
    "    def __init__(self, fpaths, scale=9.806, test=False):\n",
    "        super(FOGDataset, self).__init__()\n",
    "        tm = time.time()\n",
    "        self.test = test\n",
    "        self.fpaths = fpaths\n",
    "        self.f_ids = [os.path.basename(f)[:-4] for f in self.fpaths]\n",
    "        self.curr_df_idx = 0\n",
    "        self.curr_row_idx = 0\n",
    "        self.dfs = [np.array(pd.read_csv(f)) for f in fpaths]\n",
    "        self.end_indices = []\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.length = 0\n",
    "        for df in self.dfs:\n",
    "            self.length += df.shape[0]\n",
    "            self.end_indices.append(self.length)\n",
    "            \n",
    "        print(f\"Dataset initialized in {time.time() - tm} secs!\")\n",
    "        \n",
    "    def pad(self, df, time_start):\n",
    "        if df.shape[0] == cfg.window_size:\n",
    "            return df\n",
    "        \n",
    "        npad = cfg.window_size - df.shape[0]\n",
    "        padzeros = np.zeros((npad, 3))\n",
    "        if time_start <= 0:\n",
    "            df = np.concatenate((padzeros, df), axis=0)\n",
    "        else:\n",
    "            df = np.concatenate((df, padzeros), axis=0)\n",
    "        return df\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        for i,e in enumerate(self.end_indices):\n",
    "            if index >= e:\n",
    "                continue\n",
    "            df_idx = i\n",
    "            break\n",
    "            \n",
    "        curr_df = self.dfs[i]\n",
    "        row_idx = curr_df.shape[0] - (self.end_indices[i] - index)\n",
    "        _id = self.f_ids[i] + \"_\" + str(row_idx)\n",
    "        \n",
    "        x = self.pad(curr_df[row_idx-cfg.window_past:row_idx+cfg.window_future, 1:4], row_idx-cfg.window_past )\n",
    "        x = torch.tensor(x)/self.scale\n",
    "        \n",
    "        if self.test == True:\n",
    "            return _id, x\n",
    "        \n",
    "        y = curr_df[row_idx, -3:].astype('float')\n",
    "        y = torch.tensor(y)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\"\"\"\n",
    "class FOGDataset(Dataset):\n",
    "    def __init__(self, fpaths, scale=9.806, split=\"train\"):\n",
    "        super(FOGDataset, self).__init__()\n",
    "        tm = time.time()\n",
    "        self.split = split\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.fpaths = fpaths\n",
    "        self.dfs = [self.read(f[0], f[1]) for f in fpaths]\n",
    "        self.f_ids = [os.path.basename(f[0])[:-4] for f in self.fpaths]\n",
    "        \n",
    "        self.end_indices = []\n",
    "        self.shapes = []\n",
    "        _length = 0\n",
    "        for df in self.dfs:\n",
    "            self.shapes.append(df.shape[0])\n",
    "            _length += df.shape[0]\n",
    "            self.end_indices.append(_length)\n",
    "        \n",
    "        self.dfs = np.concatenate(self.dfs, axis=0).astype(np.float16)\n",
    "        self.length = self.dfs.shape[0]\n",
    "        \n",
    "        shape1 = self.dfs.shape[1]\n",
    "        \n",
    "        self.dfs = np.concatenate([np.zeros((cfg.wx*cfg.window_past, shape1)), self.dfs, np.zeros((cfg.wx*cfg.window_future, shape1))], axis=0)\n",
    "        print(f\"Dataset initialized in {time.time() - tm} secs!\")\n",
    "        gc.collect()\n",
    "        \n",
    "    def read(self, f, _type):\n",
    "        df = pd.read_csv(f)\n",
    "        if self.split == \"test\":\n",
    "            return np.array(df)\n",
    "        \n",
    "        if _type ==\"tdcs\":\n",
    "            df['Valid'] = 1\n",
    "            df['Task'] = 1\n",
    "            df['tdcs'] = 1\n",
    "        else:\n",
    "            df['tdcs'] = 0\n",
    "        \n",
    "        return np.array(df)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        if self.split == \"train\":\n",
    "            row_idx = random.randint(0, self.length-1) + cfg.wx*cfg.window_past\n",
    "        elif self.split == \"test\":\n",
    "            for i,e in enumerate(self.end_indices):\n",
    "                if index >= e:\n",
    "                    continue\n",
    "                df_idx = i\n",
    "                break\n",
    "\n",
    "            row_idx_true = self.shapes[df_idx] - (self.end_indices[df_idx] - index)\n",
    "            _id = self.f_ids[df_idx] + \"_\" + str(row_idx_true)\n",
    "            row_idx = index + cfg.wx*cfg.window_past\n",
    "        else:\n",
    "            row_idx = index + cfg.wx*cfg.window_past\n",
    "            \n",
    "        #scale = 9.806 if self.dfs[row_idx, -1] == 1 else 1.0\n",
    "        x = self.dfs[row_idx - cfg.wx*cfg.window_past : row_idx + cfg.wx*cfg.window_future, 1:4]\n",
    "        x = x[::cfg.wx, :][::-1, :]\n",
    "        x = torch.tensor(x.astype('float'))#/scale\n",
    "        \n",
    "        t = self.dfs[row_idx, -3]*self.dfs[row_idx, -2]\n",
    "        \n",
    "        if self.split == \"test\":\n",
    "            return _id, x, t\n",
    "        \n",
    "        y = self.dfs[row_idx, 4:7].astype('float')\n",
    "        y = torch.tensor(y)\n",
    "        \n",
    "        return x, y, t\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Analysis of positive instances in each fold of our CV folds\\n\\nSH = []\\nT = []\\nW = []\\n\\n# Here I am using the metadata file available during training. Since the code will run again during submission, if \\n# I used the usual file from the competition folder, it would have been updated with the test files too.\\nmetadata = pd.read_csv(f\"{cfg.DATA_DIR}tdcsfog_metadata.csv\")\\n\\nfor f in tqdm(metadata[\\'Id\\']):\\n    fpath = f\"{cfg.TRAIN_DIR}tdcsfog/{f}.csv\"\\n    df = pd.read_csv(fpath)\\n    \\n    SH.append(np.sum(df[\\'StartHesitation\\']))\\n    T.append(np.sum(df[\\'Turn\\']))\\n    W.append(np.sum(df[\\'Walking\\']))\\n\\nmetadata[\\'SH\\'] = SH\\nmetadata[\\'T\\'] = T\\nmetadata[\\'W\\'] = W\\n\\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata[\\'Id\\'], y=[1]*len(metadata), groups=metadata[\\'Subject\\'])):\\n    print(f\"Fold = {i}\")\\n    train_ids = metadata.loc[train_index, \\'Id\\']\\n    valid_ids = metadata.loc[valid_index, \\'Id\\']\\n    \\n    print(f\"Length of Train = {len(train_ids)}, Length of trainid = {len(valid_index)}\")\\n    n1_sum = metadata.loc[train_index, \\'SH\\'].sum()\\n    n2_sum = metadata.loc[train_index, \\'T\\'].sum()\\n    n3_sum = metadata.loc[train_index, \\'W\\'].sum()\\n    print(f\"Train classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\\n    \\n    n1_sum = metadata.loc[valid_index, \\'SH\\'].sum()\\n    n2_sum = metadata.loc[valid_index, \\'T\\'].sum()\\n    n3_sum = metadata.loc[valid_index, \\'W\\'].sum()\\n    print(f\"Valid classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\\n    \\n# # FOLD 2 is the most well balanced\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Analysis of positive instances in each fold of our CV folds\n",
    "\n",
    "SH = []\n",
    "T = []\n",
    "W = []\n",
    "\n",
    "# Here I am using the metadata file available during training. Since the code will run again during submission, if \n",
    "# I used the usual file from the competition folder, it would have been updated with the test files too.\n",
    "metadata = pd.read_csv(f\"{cfg.DATA_DIR}tdcsfog_metadata.csv\")\n",
    "\n",
    "for f in tqdm(metadata['Id']):\n",
    "    fpath = f\"{cfg.TRAIN_DIR}tdcsfog/{f}.csv\"\n",
    "    df = pd.read_csv(fpath)\n",
    "    \n",
    "    SH.append(np.sum(df['StartHesitation']))\n",
    "    T.append(np.sum(df['Turn']))\n",
    "    W.append(np.sum(df['Walking']))\n",
    "\n",
    "metadata['SH'] = SH\n",
    "print(SH)\n",
    "metadata['T'] = T\n",
    "print(T)\n",
    "metadata['W'] = W\n",
    "print(W)\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n",
    "    print(f\"Fold = {i}\")\n",
    "    train_ids = metadata.loc[train_index, 'Id']\n",
    "    valid_ids = metadata.loc[valid_index, 'Id']\n",
    "    \n",
    "    print(f\"Length of Train = {len(train_ids)}, Length of trainid = {len(valid_index)}\")\n",
    "    n1_sum = metadata.loc[train_index, 'SH'].sum()\n",
    "    n2_sum = metadata.loc[train_index, 'T'].sum()\n",
    "    n3_sum = metadata.loc[train_index, 'W'].sum()\n",
    "    print(f\"Train classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n",
    "    \n",
    "    n1_sum = metadata.loc[valid_index, 'SH'].sum()\n",
    "    n2_sum = metadata.loc[valid_index, 'T'].sum()\n",
    "    n3_sum = metadata.loc[valid_index, 'W'].sum()\n",
    "    print(f\"Valid classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n",
    "    \n",
    "# # FOLD 2 is the most well balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 2\n",
      "Length of Train = 703, Length of Valid = 130\n",
      "Fold = 2\n",
      "Length of Train = 108, Length of Valid = 29\n"
     ]
    }
   ],
   "source": [
    "# The actual train-test split (based on Fold 2)\n",
    "\n",
    "metadata = pd.read_csv(f\"{cfg.DATA_DIR}tdcsfog_metadata.csv\")\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n",
    "    if i != 2:\n",
    "        continue\n",
    "    print(f\"Fold = {i}\")\n",
    "    train_ids = metadata.loc[train_index, 'Id']\n",
    "    valid_ids = metadata.loc[valid_index, 'Id']\n",
    "    print(f\"Length of Train = {len(train_ids)}, Length of Valid = {len(valid_ids)}\")\n",
    "    \n",
    "    if i == 2:\n",
    "        break\n",
    "        \n",
    "train_fpaths_tdcs = [f\"{cfg.DATA_DIR}train/tdcsfog/{_id}.csv\" for _id in train_ids if os.path.exists(f\"{cfg.DATA_DIR}train/tdcsfog/{_id}.csv\")]\n",
    "valid_fpaths_tdcs = [f\"{cfg.DATA_DIR}train/tdcsfog/{_id}.csv\" for _id in valid_ids if os.path.exists(f\"{cfg.DATA_DIR}train/tdcsfog/{_id}.csv\")]\n",
    "\n",
    "\n",
    "metadata = pd.read_csv(f\"{cfg.DATA_DIR}defog_metadata.csv\")\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n",
    "    if i != 2:\n",
    "        continue\n",
    "    print(f\"Fold = {i}\")\n",
    "    train_ids = metadata.loc[train_index, 'Id']\n",
    "    valid_ids = metadata.loc[valid_index, 'Id']\n",
    "    print(f\"Length of Train = {len(train_ids)}, Length of Valid = {len(valid_ids)}\")\n",
    "    \n",
    "    if i == 2:\n",
    "        break\n",
    "        \n",
    "train_fpaths_de = [f\"{cfg.DATA_DIR}train/defog/{_id}.csv\" for _id in train_ids if os.path.exists(f\"{cfg.DATA_DIR}train/defog/{_id}.csv\")]\n",
    "valid_fpaths_de = [f\"{cfg.DATA_DIR}train/defog/{_id}.csv\" for _id in valid_ids if os.path.exists(f\"{cfg.DATA_DIR}train/defog/{_id}.csv\")]\n",
    "\n",
    "train_fpaths = [(f, 'de') for f in train_fpaths_de] + [(f, 'tdcs') for f in train_fpaths_tdcs]\n",
    "valid_fpaths = [(f, 'de') for f in valid_fpaths_de] + [(f, 'tdcs') for f in valid_fpaths_tdcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized in 10.868338108062744 secs!\n"
     ]
    }
   ],
   "source": [
    "fog_train = FOGDataset(train_fpaths)\n",
    "fog_train_loader = DataLoader(fog_train, batch_size=cfg.batch_size, shuffle=True) #, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized in 1.8871819972991943 secs!\n"
     ]
    }
   ],
   "source": [
    "fog_valid = FOGDataset(valid_fpaths)\n",
    "fog_valid_loader = DataLoader(fog_valid, batch_size=cfg.batch_size) #, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5963939\n",
      "Number of batches: 5825\n",
      "Batch size: 1024\n",
      "Total size: 5964800\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", fog_train.__len__())\n",
    "print(\"Number of batches:\", len(fog_train_loader))\n",
    "print(\"Batch size:\", fog_train_loader.batch_size)\n",
    "print(\"Total size:\", len(fog_train_loader) * fog_train_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1098733\n",
      "Number of batches: 1073\n",
      "Batch size: 1024\n",
      "Total size: 1098752\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", fog_valid.__len__())\n",
    "print(\"Number of batches:\", len(fog_valid_loader))\n",
    "print(\"Batch size:\", fog_valid_loader.batch_size)\n",
    "print(\"Total size:\", len(fog_valid_loader) * fog_valid_loader.batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _block(in_features, out_features, drop_rate):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features),\n",
    "        nn.BatchNorm1d(out_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(drop_rate)\n",
    "    )\n",
    "\n",
    "class FOGModel(nn.Module):\n",
    "    def __init__(self, state=\"finetune\", p=cfg.model_dropout, dim=cfg.model_hidden, nblocks=cfg.model_nblocks):\n",
    "        super(FOGModel, self).__init__()\n",
    "        self.hparams = {}\n",
    "        self.state = state\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.in_layer = nn.Linear(cfg.window_size*3, dim)\n",
    "        self.blocks = nn.Sequential(*[_block(dim, dim, p) for _ in range(nblocks)])\n",
    "\n",
    "        self.out_layer_pretrain = nn.Linear(dim, cfg.window_future * 3)\n",
    "        self.out_layer_finetune = nn.Linear(dim, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, cfg.window_size*3)\n",
    "        x = self.in_layer(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        if self.state == \"pretrain\":\n",
    "            x = self.out_layer_pretrain(x)\n",
    "        else:\n",
    "            x = self.out_layer_finetune(x)\n",
    "        return x\n",
    "\n",
    "class FOGTransformer(nn.Module):\n",
    "    def __init__(self, state=\"finetuning\", p=cfg.model_dropout, dim=cfg.model_hidden, nblocks=cfg.model_nblocks):\n",
    "        super(FOGTransformer, self).__init__()\n",
    "        self.hparams = {}\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.in_layer = nn.Linear(cfg.window_size*3, dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=8, dim_feedforward=dim)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=nblocks, mask_check=False)\n",
    "\n",
    "        if state == \"pretrain\":\n",
    "            self.out_layer = nn.Linear(dim, cfg.window_future * 3)\n",
    "        elif state == \"finetune\":\n",
    "            self.out_layer = nn.Linear(dim, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, cfg.window_size*3)\n",
    "        x = self.in_layer(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,205,632 trainable parameters\n",
      "The model has 327,195 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(FOGTransformer()):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(FOGModel()):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntdcsfog_train_loader = DataLoader(tdcsfog_train, batch_size=cfg.batch_size, shuffle=True)\\n\\nmodel = FOGModel()\\noptimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\\ncriterion = nn.BCEWithLogitsLoss()\\nsoft = nn.Softmax(dim=-1)\\n\\n# def average_precision_score(y_true, y_pred):\\n#         # average precision with pytorch\\n#         y = y_true.argmax(dim=-1)\\n#         average_precision = AveragePrecision(task=\"multiclass\", num_classes=3, average=None)\\n#         return average_precision(y_pred, y)\\n\\ndef train(model, optimizer, criterion, train_loader):\\n    for x, y in tqdm(train_loader):\\n        # print(y)\\n        # print(x.shape, y.shape)\\n        #ic(x, y)\\n        # single forward pass\\n        # cast x to the correct data type\\n        x = x.float()\\n        y_hat = model(x)\\n        print(y_hat)\\n        # print(soft(y_hat))\\n        # print(y_hat.shape)\\n        # print(y_hat.argmax(dim=-1))\\n        # calculate loss\\n        loss = criterion(y_hat, y)\\n        acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean()\\n        # calculate gradients\\n        loss.backward()\\n        # update weights\\n        optimizer.step()\\n        # print out the loss using ic\\n        #print(loss.item())\\n        #print(acc.item())\\n        print(y)\\n\\n        with torch.no_grad():\\n            print(average_precision_score(y, y_hat, average=None))\\n            print(multiclass_average_precision(y_hat, y.argmax(-1), num_classes=3, average=None))\\n        break\\n\\ndef validation(model, criterion, valid_loader):\\n    lol = []\\n    lil = []\\n    c = 0\\n    for x, y in tqdm(valid_loader):\\n        # single forward pass\\n        # cast x to the correct data type\\n        x = x.float()\\n        # disable gradient calculation\\n        with torch.no_grad():\\n            y_hat = model(x)\\n        print(y_hat)\\n        #print(y_hat.argmax(dim=-1))\\n        print(y)\\n        lil = lil + y_hat.tolist()\\n        lol = lol + y.tolist()\\n        #print(y.argmax(dim=-1))\\n\\n        # calculate loss\\n        loss = criterion(y_hat, y)\\n        acc = (lil.argmax(dim=-1) == lol.argmax(dim=-1)).float().mean()\\n        # print out the loss using ic\\n        #print(loss.item())\\n        #print(acc.item())\\n        print(average_precision_score(y, y_hat, average=None))\\n        print(multiclass_average_precision(y_hat, y.argmax(-1), num_classes=3, average=None))\\n        c += 1\\n        if c == 3:\\n            break\\n    print(lil)\\n    print(lol)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tdcsfog_train_loader = DataLoader(tdcsfog_train, batch_size=cfg.batch_size, shuffle=True)\n",
    "\n",
    "model = FOGModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "soft = nn.Softmax(dim=-1)\n",
    "\n",
    "# def average_precision_score(y_true, y_pred):\n",
    "#         # average precision with pytorch\n",
    "#         y = y_true.argmax(dim=-1)\n",
    "#         average_precision = AveragePrecision(task=\"multiclass\", num_classes=3, average=None)\n",
    "#         return average_precision(y_pred, y)\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader):\n",
    "    for x, y in tqdm(train_loader):\n",
    "        # print(y)\n",
    "        # print(x.shape, y.shape)\n",
    "        #ic(x, y)\n",
    "        # single forward pass\n",
    "        # cast x to the correct data type\n",
    "        x = x.float()\n",
    "        y_hat = model(x)\n",
    "        print(y_hat)\n",
    "        # print(soft(y_hat))\n",
    "        # print(y_hat.shape)\n",
    "        # print(y_hat.argmax(dim=-1))\n",
    "        # calculate loss\n",
    "        loss = criterion(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean()\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # print out the loss using ic\n",
    "        #print(loss.item())\n",
    "        #print(acc.item())\n",
    "        print(y)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(average_precision_score(y, y_hat, average=None))\n",
    "            print(multiclass_average_precision(y_hat, y.argmax(-1), num_classes=3, average=None))\n",
    "        break\n",
    "\n",
    "def validation(model, criterion, valid_loader):\n",
    "    lol = []\n",
    "    lil = []\n",
    "    c = 0\n",
    "    for x, y in tqdm(valid_loader):\n",
    "        # single forward pass\n",
    "        # cast x to the correct data type\n",
    "        x = x.float()\n",
    "        # disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(x)\n",
    "        print(y_hat)\n",
    "        #print(y_hat.argmax(dim=-1))\n",
    "        print(y)\n",
    "        lil = lil + y_hat.tolist()\n",
    "        lol = lol + y.tolist()\n",
    "        #print(y.argmax(dim=-1))\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(y_hat, y)\n",
    "        acc = (lil.argmax(dim=-1) == lol.argmax(dim=-1)).float().mean()\n",
    "        # print out the loss using ic\n",
    "        #print(loss.item())\n",
    "        #print(acc.item())\n",
    "        print(average_precision_score(y, y_hat, average=None))\n",
    "        print(multiclass_average_precision(y_hat, y.argmax(-1), num_classes=3, average=None))\n",
    "        c += 1\n",
    "        if c == 3:\n",
    "            break\n",
    "    print(lil)\n",
    "    print(lol)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FOGModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, optimizer_name, optimizer_hparams):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model_name - Name of the model to run. Used for creating the model (see function below)\n",
    "            model_hparams - Hyperparameters for the model, as dictionary.\n",
    "            optimizer_name - Name of the optimizer to use. Currently supported: Adam, SGD\n",
    "            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "        # Create model\n",
    "        self.model = model\n",
    "        # Create loss module\n",
    "        self.loss_module = nn.BCEWithLogitsLoss()\n",
    "        # Example input for visualizing the graph in Tensorboard\n",
    "        self.example_input_array = torch.zeros((1, cfg.window_size, 3), dtype=torch.float32)\n",
    "        self.val_true = None\n",
    "        self.val_pred = None\n",
    "\n",
    "    def forward(self, past):\n",
    "        # Forward function that is run when visualizing the graph\n",
    "        return self.model(past)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We will support Adam or SGD as optimizers.\n",
    "        if self.hparams.optimizer_name == \"Adam\":\n",
    "            # AdamW is Adam with a correct implementation of weight decay (see here for details: https://arxiv.org/pdf/1711.05101.pdf)\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        elif self.hparams.optimizer_name == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        else:\n",
    "            assert False, f\"Unknown optimizer: \\\"{self.hparams.optimizer_name}\\\"\"\n",
    "\n",
    "        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=cfg.milestones, gamma=cfg.gamma)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "        x, y, t = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        preds = self.model(x)\n",
    "        loss = self.loss_module(preds, y)\n",
    "        acc = (preds.argmax(dim=-1) == y.argmax(dim=-1)).float().mean()\n",
    "        self.log('train_acc', acc)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ap = self.average_precision_score(y, preds)\n",
    "        self.log('train_ap0', ap[0])\n",
    "        self.log('train_ap1', ap[1])\n",
    "        self.log('train_ap2', ap[2])\n",
    "        self.log('train_ap', sum(ap)/3)\n",
    "\n",
    "        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
    "        #self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss  # Return tensor to call \".backward\" on\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_precision = self.trainer.logged_metrics['train_ap0'].nanmean()\n",
    "        self.log('train0_precision', avg_precision)\n",
    "        avg_precision = self.trainer.logged_metrics['train_ap1'].nanmean()\n",
    "        self.log('train1_precision', avg_precision)\n",
    "        avg_precision = self.trainer.logged_metrics['train_ap2'].nanmean()\n",
    "        self.log('train2_precision', avg_precision)\n",
    "        avg_precision = self.trainer.logged_metrics['train_ap'].nanmean()\n",
    "        self.log('avg_train_precision', avg_precision)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, t = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        preds = self.model(x)\n",
    " \n",
    "        if self.val_true is None:\n",
    "            self.val_true = y\n",
    "            self.val_pred = preds\n",
    "        else:\n",
    "            self.val_true = torch.cat((self.val_true, y), dim=0)\n",
    "            self.val_pred = torch.cat((self.val_pred, preds), dim=0)\n",
    "\n",
    "        # By default logs it per epoch (weighted average over batches)\n",
    "        # with torch.no_grad():\n",
    "        #     ap = self.average_precision_score(future, preds)\n",
    "        # self.log('val_ap0', ap[0])\n",
    "        # self.log('val_ap1', ap[1])\n",
    "        # self.log('val_ap2', ap[2])\n",
    "        # self.log('val_ap', sum(ap)/3)\n",
    "        # self.log('val_ap33', sum(ap)/3, on_step=True)\n",
    "        loss = self.loss_module(preds, y)\n",
    "        self.log('val_loss', loss, prog_bar=True, sync_dist=True)\n",
    "        return {'loss': loss, 'y_pred': preds, 'y': y}\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # avg_precision = self.trainer.logged_metrics['val_ap0'].nanmean()\n",
    "        # self.log('val0_precision', avg_precision)\n",
    "        # avg_precision = self.trainer.logged_metrics['val_ap1'].nanmean()\n",
    "        # self.log('val1_precision', avg_precision)\n",
    "        # avg_precision = self.trainer.logged_metrics['val_ap2'].nanmean()\n",
    "        # self.log('val2_precision', avg_precision)\n",
    "        # avg_precision = self.trainer.logged_metrics['val_ap'].nanmean()\n",
    "        # self.log('avg_val_precision', avg_precision)\n",
    "\n",
    "        acc = (self.val_true.argmax(dim=-1) == self.val_pred.argmax(dim=-1)).float().mean()\n",
    "        self.log('val_acc', acc)\n",
    "        avg_precision = self.average_precision_score(self.val_true, self.val_pred)\n",
    "        self.log('val0_precision', avg_precision[0])\n",
    "        self.log('val1_precision', avg_precision[1])\n",
    "        self.log('val2_precision', avg_precision[2])\n",
    "        self.log('avg_val_precision', sum(avg_precision)/3)\n",
    "        self.val_true = None\n",
    "        self.val_pred = None\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, t = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        preds = self.model(x)\n",
    " \n",
    "        loss = self.loss_module(preds, y)\n",
    "        if self.val_true is None:\n",
    "            self.val_true = y\n",
    "            self.val_pred = preds\n",
    "        else:\n",
    "            self.val_true = torch.cat((self.val_true, y), dim=0)\n",
    "            self.val_pred = torch.cat((self.val_pred, preds), dim=0)\n",
    "        return {'loss': loss, 'y_pred': preds, 'y': y}\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        acc = (self.val_true.argmax(dim=-1) == self.val_pred.argmax(dim=-1)).float().mean()\n",
    "        self.log('val_acc', acc)\n",
    "        avg_precision = self.average_precision_score(self.val_true, self.val_pred)\n",
    "        self.log('val0_precision', avg_precision[0])\n",
    "        self.log('val1_precision', avg_precision[1])\n",
    "        self.log('val2_precision', avg_precision[2])\n",
    "        self.log('avg_val_precision', sum(avg_precision)/3)\n",
    "        self.val_true = None\n",
    "        self.val_pred = None\n",
    "    \n",
    "    def average_precision_score(self, y_true, y_pred):\n",
    "        target = y_true.argmax(dim=-1)\n",
    "        return multiclass_average_precision(y_pred, target, num_classes=3, average=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrixCallback(pl.Callback):\n",
    "    def __init__(self, num_classes, task=\"multiclass\"):\n",
    "        super().__init__()\n",
    "        self.conf_matrix = torchmetrics.ConfusionMatrix(num_classes=num_classes, task=task).to(cfg.device)\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx = 0):\n",
    "        # Get the predicted labels and ground truth labels from the batch\n",
    "        y_pred, y_true = outputs['y_pred'].argmax(dim=-1), outputs['y'].argmax(dim=-1)\n",
    "\n",
    "        # Update the confusion matrix with the current batch\n",
    "        self.conf_matrix.update(y_pred, y_true)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Compute the confusion matrix for the entire validation set\n",
    "        matrix = self.conf_matrix.compute().detach().cpu()\n",
    "\n",
    "        # Print the confusion matrix\n",
    "        print('Confusion matrix:')\n",
    "        print(matrix)\n",
    "\n",
    "    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx = 0):\n",
    "        # Get the predicted labels and ground truth labels from the batch\n",
    "        y_pred, y_true = outputs['y_pred'].argmax(dim=-1), outputs['y'].argmax(dim=-1)\n",
    "\n",
    "        # Update the confusion matrix with the current batch\n",
    "        self.conf_matrix.update(y_pred, y_true)\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        # Compute the confusion matrix for the entire test set\n",
    "        matrix = self.conf_matrix.compute().detach().cpu()\n",
    "\n",
    "        # Print the confusion matrix\n",
    "        print('Confusion matrix:')\n",
    "        print(matrix)\n",
    "        \n",
    "def train_model(module, model, train_loader, val_loader, test_loader, save_name = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    conf_matrix_callback = ConfusionMatrixCallback(num_classes=3, task=\"multiclass\")\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(cfg.CHECKPOINT_PATH, save_name),                          # Where to save models\n",
    "                         accelerator=\"gpu\" if str(cfg.device).startswith(\"cuda\") else \"cpu\",                     # We run on a GPU (if possible)\n",
    "                         devices=torch.cuda.device_count() if str(cfg.device).startswith(\"cuda\") else 1,                                                                          # How many GPUs/CPUs we want to use (1 is enough for the notebooks)\n",
    "                         max_epochs=cfg.num_epochs,                                                                     # How many epochs to train for if no patience is set\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"avg_val_precision\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "                                    LearningRateMonitor(\"epoch\"), conf_matrix_callback],                                           # Log learning rate every epoch\n",
    "                         enable_progress_bar=True,                                                          # Set to False if you do not want a progress bar\n",
    "                         logger = True,\n",
    "                         #strategy=DDPStrategy(find_unused_parameters=True),\n",
    "                         val_check_interval=0.5,\n",
    "                         log_every_n_steps=50)                                                           \n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = True\n",
    "\n",
    "    # log hyperparameters, including model and custom parameters\n",
    "    model.hparams.update(cfg.hparams)\n",
    "    del model.hparams[\"milestones\"] \n",
    "    trainer.logger.log_metrics(model.hparams)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(cfg.CHECKPOINT_PATH, save_name + f\"/{cfg.model_hidden}_{cfg.model_nblocks}_final.pt\")\n",
    "    print(f\"Pretrained file {pretrained_filename}\")\n",
    "    if cfg.hparams[\"use_pretrained\"] and os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model.load_state_dict(torch.load(pretrained_filename)) # Automatically loads the model with the saved hyperparameters\n",
    "    lmodel = module(model, **kwargs)\n",
    "    \n",
    "    pl.seed_everything(42) # To be reproducable\n",
    "    trainer.fit(lmodel, train_loader, val_loader)\n",
    "    print(f\"Best model path {trainer.checkpoint_callback.best_model_path}\")\n",
    "    lmodel = module.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation set\n",
    "    val_result = trainer.test(lmodel, val_loader, verbose=False)\n",
    "    result = {\n",
    "                \"val_ap\": val_result[0][\"avg_val_precision\"],\n",
    "                \"SH\": val_result[0][\"val0_precision\"],\n",
    "                \"T\": val_result[0][\"val1_precision\"],\n",
    "                \"W\": val_result[0][\"val2_precision\"]\n",
    "            }\n",
    "\n",
    "    return lmodel, trainer, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/taa/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 42\n",
      "\n",
      "  | Name        | Type              | Params | In sizes   | Out sizes\n",
      "---------------------------------------------------------------------------\n",
      "0 | model       | FOGModel          | 327 K  | [1, 32, 3] | [1, 3]   \n",
      "1 | loss_module | BCEWithLogitsLoss | 0      | ?          | ?        \n",
      "---------------------------------------------------------------------------\n",
      "327 K     Trainable params\n",
      "0         Non-trainable params\n",
      "327 K     Total params\n",
      "1.309     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained file ../checkpoints/FOGModel/512_1_final.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813d32177b2a41eab8c80567b21765d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taa/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "tensor([[791, 320, 265],\n",
      "        [387, 147,  91],\n",
      "        [ 11,  28,   8]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taa/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2467cddd87db4bdf802e205d0ebe5d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbd516a47b94f6280a9629ea8ecffca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "tensor([[  2951, 726398,    636],\n",
      "        [  4108, 342366,    130],\n",
      "        [    77,  24100,     15]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc25b883126c41a49ddfa9d6192a8116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "tensor([[  11546, 1442901,    4195],\n",
      "        [  11774,  680559,     213],\n",
      "        [    388,   47914,      24]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model path ../checkpoints/FOGModel/lightning_logs/version_63/checkpoints/epoch=0-step=5824.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taa/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b3a4821e2f42e89eebbcddfcdfe9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "tensor([[  20120, 2159292,    7729],\n",
      "        [  19457, 1018985,     302],\n",
      "        [    686,   71644,      32]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'sort_keys' is an invalid keyword argument for print()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m FOGModel()\n\u001b[1;32m      2\u001b[0m model, trainer, result \u001b[39m=\u001b[39m train_model(FOGModule, model, fog_train_loader, fog_valid_loader, fog_valid_loader, save_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFOGModel\u001b[39m\u001b[39m\"\u001b[39m, optimizer_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m\"\u001b[39m, optimizer_hparams\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: cfg\u001b[39m.\u001b[39mlr, \u001b[39m\"\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m\"\u001b[39m: cfg\u001b[39m.\u001b[39mgamma})\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39;49m(json\u001b[39m.\u001b[39;49mdumps(cfg\u001b[39m.\u001b[39;49mhparams), sort_keys\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, indent\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39mdumps(result), sort_keys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'sort_keys' is an invalid keyword argument for print()"
     ]
    }
   ],
   "source": [
    "model = FOGModel()\n",
    "model, trainer, result = train_model(FOGModule, model, fog_train_loader, fog_valid_loader, fog_valid_loader, save_name=\"FOGModel\", optimizer_name=\"Adam\", optimizer_hparams={\"lr\": cfg.lr, \"weight_decay\": cfg.gamma})\n",
    "print(json.dumps(cfg.hparams), sort_keys=True, indent=4)\n",
    "print(json.dumps(result), sort_keys=True, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized in 0.24955105781555176 secs!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b610da5765e41ec979525639551ef68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286370, 4)\n",
      "(286370, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>StartHesitation</th>\n",
       "      <th>Turn</th>\n",
       "      <th>Walking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003f117e14_0</td>\n",
       "      <td>-0.77435</td>\n",
       "      <td>-0.29401</td>\n",
       "      <td>-0.57797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f117e14_1</td>\n",
       "      <td>-0.85959</td>\n",
       "      <td>-0.35410</td>\n",
       "      <td>-0.63248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003f117e14_2</td>\n",
       "      <td>-0.90993</td>\n",
       "      <td>-0.40147</td>\n",
       "      <td>-0.68323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003f117e14_3</td>\n",
       "      <td>-0.85903</td>\n",
       "      <td>-0.40261</td>\n",
       "      <td>-0.70406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003f117e14_4</td>\n",
       "      <td>-0.76579</td>\n",
       "      <td>-0.40145</td>\n",
       "      <td>-0.66563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id  StartHesitation     Turn  Walking\n",
       "0  003f117e14_0         -0.77435 -0.29401 -0.57797\n",
       "1  003f117e14_1         -0.85959 -0.35410 -0.63248\n",
       "2  003f117e14_2         -0.90993 -0.40147 -0.68323\n",
       "3  003f117e14_3         -0.85903 -0.40261 -0.70406\n",
       "4  003f117e14_4         -0.76579 -0.40145 -0.66563"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FOGModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "model.to(cfg.device)\n",
    "model.eval()\n",
    "\n",
    "test_defog_paths = glob.glob(f\"{cfg.DATA_DIR}test/defog/*.csv\")\n",
    "test_tdcsfog_paths = glob.glob(f\"{cfg.DATA_DIR}test/tdcsfog/*.csv\")\n",
    "test_fpaths = [(f, 'de') for f in test_defog_paths] + [(f, 'tdcs') for f in test_tdcsfog_paths]\n",
    "\n",
    "test_dataset = FOGDataset(test_fpaths, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size) #, num_workers=cfg.num_workers)\n",
    "\n",
    "ids = []\n",
    "preds = []\n",
    "\n",
    "for _id, x, _ in tqdm(test_loader):\n",
    "    x = x.to(cfg.device).float()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)*0.1\n",
    "\n",
    "    ids.extend(_id)\n",
    "    preds.extend(list(np.nan_to_num(y_pred.cpu().numpy())))\n",
    "\n",
    "sample_submission = pd.read_csv(f\"{cfg.DATA_DIR}sample_submission.csv\")\n",
    "print(sample_submission.shape)\n",
    "\n",
    "preds = np.array(preds)\n",
    "submission = pd.DataFrame({'Id': ids, 'StartHesitation': np.round(preds[:,0],5), \\\n",
    "                           'Turn': np.round(preds[:,1],5), 'Walking': np.round(preds[:,2],5)})\n",
    "\n",
    "submission = pd.merge(sample_submission[['Id']], submission, how='left', on='Id').fillna(0.0)\n",
    "submission.to_csv(f\"submission.csv\", index=False)\n",
    "\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parkinson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

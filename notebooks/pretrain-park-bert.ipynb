{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchmetrics.functional.classification import multiclass_average_precision\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': '__main__', 'KAGGLE': False, 'ROOT_READ': '../', 'ROOT_WRITE': '../', 'DATA_DIR': '../tlvmc-parkinsons-freezing-gait-prediction/', 'TRAIN_DIR': '../tlvmc-parkinsons-freezing-gait-prediction/train/', 'CHECKPOINT_PATH': '../checkpoints/', 'PARAMS_PATH': './pretrain-config.json', 'f': <_io.TextIOWrapper name='./pretrain-config.json' mode='r' encoding='UTF-8'>, 'hparams': {'batch_size': 1024, 'window_size': 32, 'window_future': 8, 'wx': 8, 'model_dropout': 0.2, 'model_hidden': 512, 'model_nblocks': 1, 'model_nhead': 1, 'lr': 0.00015, 'milestones': [5, 10, 15, 20], 'gamma': 3e-05, 'num_epochs': 4}, 'batch_size': 1024, 'window_size': 32, 'window_future': 8, 'window_past': 24, 'wx': 8, 'model_dropout': 0.2, 'model_hidden': 512, 'model_nblocks': 1, 'model_nhead': 1, 'lr': 0.00015, 'milestones': [5, 10, 15, 20], 'gamma': 3e-05, 'num_epochs': 4, 'device': 'cpu', 'num_workers': 8, 'feature_list': ['AccV', 'AccML', 'AccAP'], 'label_list': ['StartHesitation', 'Turn', 'Walking'], '__dict__': <attribute '__dict__' of 'Config' objects>, '__weakref__': <attribute '__weakref__' of 'Config' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    KAGGLE = False\n",
    "    ROOT_READ = '../'\n",
    "    ROOT_WRITE = '../'\n",
    "    if KAGGLE:\n",
    "        ROOT_READ = '/kaggle/input/'\n",
    "        ROOT_WRITE = '/kaggle/working/'\n",
    "    DATA_DIR = f'{ROOT_READ}tlvmc-parkinsons-freezing-gait-prediction/'\n",
    "    TRAIN_DIR = f'{ROOT_READ}tlvmc-parkinsons-freezing-gait-prediction/train/'\n",
    "    CHECKPOINT_PATH = f'{ROOT_WRITE}checkpoints/'\n",
    "    PARAMS_PATH = f'./pretrain-config.json'\n",
    "    if KAGGLE:\n",
    "        PARAMS_PATH = '/' # TODO\n",
    "\n",
    "\n",
    "    with open(PARAMS_PATH) as f:\n",
    "        hparams = json.load(f)\n",
    "\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    window_size = hparams[\"window_size\"]\n",
    "    window_future = hparams[\"window_future\"]\n",
    "    window_past = window_size - window_future\n",
    "    wx = hparams[\"wx\"]\n",
    "\n",
    "    model_dropout = hparams[\"model_dropout\"]\n",
    "    model_hidden = hparams[\"model_hidden\"]\n",
    "    model_nblocks = hparams[\"model_nblocks\"]\n",
    "    model_nhead = hparams[\"model_nhead\"]\n",
    "\n",
    "    lr = hparams[\"lr\"]\n",
    "    milestones = hparams[\"milestones\"]\n",
    "    gamma = hparams[\"gamma\"]\n",
    "\n",
    "    num_epochs = hparams[\"num_epochs\"]\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    num_workers = os.cpu_count()\n",
    "\n",
    "    feature_list = ['AccV', 'AccML', 'AccAP']\n",
    "    label_list = ['StartHesitation', 'Turn', 'Walking']\n",
    "\n",
    "cfg = Config()\n",
    "print(vars(Config))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FOGDataset(Dataset):\n",
    "    def __init__(self, fpaths, scale=9.806, split=\"train\", state=\"fine-tune\"):\n",
    "        super(FOGDataset, self).__init__()\n",
    "        tm = time.time()\n",
    "        self.split = split\n",
    "        self.state = state\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.fpaths = fpaths\n",
    "        self.dfs = [self.read(f[0], f[1]) for f in fpaths]\n",
    "        self.f_ids = [os.path.basename(f[0])[:-4] for f in self.fpaths]\n",
    "        \n",
    "        self.end_indices = []\n",
    "        self.shapes = []\n",
    "        _length = 0\n",
    "        print(\"initializing...\")\n",
    "        for df in self.dfs:\n",
    "            self.shapes.append(df.shape[0])\n",
    "            _length += df.shape[0]\n",
    "            self.end_indices.append(_length)\n",
    "            print(df.shape[0], _length)\n",
    "        \n",
    "        self.dfs = np.concatenate(self.dfs, axis=0).astype(np.float16)\n",
    "        self.length = self.dfs.shape[0]\n",
    "        \n",
    "        shape1 = self.dfs.shape[1]\n",
    "        \n",
    "        self.dfs = np.concatenate([np.zeros((cfg.wx*cfg.window_past, shape1)), self.dfs, np.zeros((cfg.wx*cfg.window_future, shape1))], axis=0)\n",
    "        print(f\"Dataset initialized in {time.time() - tm} secs!\")\n",
    "        gc.collect()\n",
    "        \n",
    "    def read(self, f, _type):\n",
    "        print(f\"Reading file {f}...\")\n",
    "        if self.state == \"pre-train\":\n",
    "            df = pd.read_parquet(f)\n",
    "        elif self.state == \"fine-tune\": \n",
    "            df = pd.read_csv(f)\n",
    "            \n",
    "        if self.split == \"test\" or self.state == \"pre-train\":\n",
    "            return np.array(df)\n",
    "        \n",
    "        if _type ==\"tdcs\":\n",
    "            df['Valid'] = 1\n",
    "            df['Task'] = 1\n",
    "            df['tdcs'] = 1\n",
    "        else:\n",
    "            df['tdcs'] = 0\n",
    "        \n",
    "        return np.array(df)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        if self.split == \"train\":\n",
    "            row_idx = random.randint(0, self.length-1) + cfg.wx*cfg.window_past\n",
    "        elif self.split == \"test\":\n",
    "            for i,e in enumerate(self.end_indices):\n",
    "                if index >= e:\n",
    "                    continue\n",
    "                df_idx = i\n",
    "                break\n",
    "\n",
    "            row_idx_true = self.shapes[df_idx] - (self.end_indices[df_idx] - index)\n",
    "            _id = self.f_ids[df_idx] + \"_\" + str(row_idx_true)\n",
    "            row_idx = index + cfg.wx*cfg.window_past\n",
    "        else:\n",
    "            row_idx = index + cfg.wx*cfg.window_past\n",
    "\n",
    "        #scale = 9.806 if self.dfs[row_idx, -1] == 1 else 1.0\n",
    "        x = self.dfs[row_idx - cfg.wx*cfg.window_past : row_idx + cfg.wx*cfg.window_future, 1:4]\n",
    "        x = x[::cfg.wx, :][::-1, :]\n",
    "        x = torch.tensor(x.astype('float'))\n",
    "        \n",
    "        t = {} #self.dfs[row_idx, -3]*self.dfs[row_idx, -2]\n",
    "        \n",
    "        y = self.dfs[row_idx + cfg.wx*cfg.window_future : row_idx + 2*cfg.wx*cfg.window_future, 1:4]\n",
    "        y = y[::cfg.wx, :][::-1, :]\n",
    "        y = torch.tensor(y.astype('float'))\n",
    "        return x, y, t\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _block(in_features, out_features, drop_rate):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features, out_features),\n",
    "        nn.BatchNorm1d(out_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(drop_rate)\n",
    "    )\n",
    "\n",
    "class FOGModel(nn.Module):\n",
    "    def __init__(self, p=cfg.model_dropout, dim=cfg.model_hidden, nblocks=cfg.model_nblocks):\n",
    "        super(FOGModel, self).__init__()\n",
    "        self.hparams = {}\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.in_layer = nn.Linear(cfg.window_size*3, dim)\n",
    "        self.blocks = nn.Sequential(*[_block(dim, dim, p) for _ in range(nblocks)])\n",
    "        self.out_layer = nn.Linear(dim, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, cfg.window_size*3)\n",
    "        x = self.in_layer(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "class FOGTransformerEncoder(nn.Module):\n",
    "    def __init__(self, state=\"finetune\", p=cfg.model_dropout, dim=cfg.model_hidden, nblocks=cfg.model_nblocks):\n",
    "        super(FOGTransformerEncoder, self).__init__()\n",
    "        self.hparams = {}\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.in_layer = nn.Linear(cfg.window_size*3, dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=cfg.model_nhead, dim_feedforward=dim)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=nblocks, mask_check=False)\n",
    "\n",
    "        if state == \"pretrain\":\n",
    "            self.out_layer = nn.Linear(dim, cfg.window_future * 3)\n",
    "        elif state == \"finetune\":\n",
    "            self.out_layer = nn.Linear(dim, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, cfg.window_size*3)\n",
    "        x = self.in_layer(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x.reshape(-1, cfg.window_future, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ../tlvmc-parkinsons-freezing-gait-prediction/temp/baac585916.parquet...\n",
      "initializing...\n",
      "60479060 60479060\n",
      "Dataset initialized in 12.762964963912964 secs!\n",
      "Dataset size: 60479060\n",
      "Number of batches: 59062\n",
      "Batch size: 1024\n",
      "Total size: 60479488\n"
     ]
    }
   ],
   "source": [
    "pretrain_paths = [(f, 'temp') for f in glob.glob(f\"{cfg.DATA_DIR}temp/*.parquet\")]\n",
    "fog_pretrain = FOGDataset(pretrain_paths, state=\"pre-train\")\n",
    "fog_train_loader = DataLoader(fog_pretrain, batch_size=cfg.batch_size, shuffle=True) #, num_workers=cfg.num_workers)\n",
    "print(\"Dataset size:\", fog_pretrain.__len__())\n",
    "print(\"Number of batches:\", len(fog_train_loader))\n",
    "print(\"Batch size:\", fog_train_loader.batch_size)\n",
    "print(\"Total size:\", len(fog_train_loader) * fog_train_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainingFogModule(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer_name, optimizer_hparams):\n",
    "        super(PreTrainingFogModule, self).__init__()\n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), **self.hparams.optimizer_hparams)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_model(module, model, train_loader, save_name = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(cfg.CHECKPOINT_PATH, save_name),                          # Where to save models\n",
    "                         accelerator=\"gpu\" if str(cfg.device).startswith(\"cuda\") else \"cpu\",                     # We run on a GPU (if possible)\n",
    "                         devices=torch.cuda.device_count() if str(cfg.device).startswith(\"cuda\") else 1,                                                                          # How many GPUs/CPUs we want to use (1 is enough for the notebooks)\n",
    "                         max_epochs=cfg.num_epochs,                                                                     # How many epochs to train for if no patience is set\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"avg_val_precision\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "                                    LearningRateMonitor(\"epoch\")],                                           # Log learning rate every epoch\n",
    "                         enable_progress_bar=True,                                                          # Set to False if you do not want a progress bar\n",
    "                         logger = True,\n",
    "                         # val_check_interval=0.5,\n",
    "                         log_every_n_steps=50)                                                           \n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = True\n",
    "\n",
    "    # log hyperparameters, including model and custom parameters\n",
    "    model.hparams.update(cfg.hparams)\n",
    "    del model.hparams[\"milestones\"] # = str(model.hparams[\"milestones\"])\n",
    "    trainer.logger.log_metrics(model.hparams)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(cfg.CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = module.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        lmodel = module(model, **kwargs)\n",
    "        trainer.fit(lmodel, train_loader)\n",
    "        lmodel = module.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    train_loss = trainer.logged_metrics[\"train_loss\"]\n",
    "    result = {\n",
    "        \"train_loss\": train_loss\n",
    "    }\n",
    "\n",
    "    return lmodel, trainer, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "Global seed set to 42\n",
      "/Users/taa/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type                  | Params\n",
      "----------------------------------------------------\n",
      "0 | model     | FOGTransformerEncoder | 3.2 M \n",
      "1 | criterion | MSELoss               | 0     \n",
      "----------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n",
      "12.872    Total estimated model params size (MB)\n",
      "\n",
      "  | Name      | Type                  | Params\n",
      "----------------------------------------------------\n",
      "0 | model     | FOGTransformerEncoder | 3.2 M \n",
      "1 | criterion | MSELoss               | 0     \n",
      "----------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n",
      "12.872    Total estimated model params size (MB)\n",
      "/Users/taa/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/loggers/tensorboard.py:191: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n",
      "/Users/taa/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a079a77fe8943e1aa725c7193d5f834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taa/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/Users/taa/Desktop/parkinson/pfgp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m FOGTransformerEncoder(state\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpretrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model, trainer, result \u001b[39m=\u001b[39m pretrain_model(PreTrainingFogModule, model, fog_train_loader, save_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFOGTransformerEncoder\u001b[39;49m\u001b[39m\"\u001b[39;49m, optimizer_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mAdam\u001b[39;49m\u001b[39m\"\u001b[39;49m, optimizer_hparams\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m: cfg\u001b[39m.\u001b[39;49mlr, \u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m: cfg\u001b[39m.\u001b[39;49mgamma})\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39mdumps(cfg\u001b[39m.\u001b[39mhparams, sort_keys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39mdumps(result, sort_keys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[33], line 35\u001b[0m, in \u001b[0;36mpretrain_model\u001b[0;34m(module, model, train_loader, save_name, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     lmodel \u001b[39m=\u001b[39m module(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     34\u001b[0m     trainer\u001b[39m.\u001b[39mfit(lmodel, train_loader)\n\u001b[0;32m---> 35\u001b[0m     lmodel \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49mload_from_checkpoint(trainer\u001b[39m.\u001b[39;49mcheckpoint_callback\u001b[39m.\u001b[39;49mbest_model_path) \u001b[39m# Load best checkpoint after training\u001b[39;00m\n\u001b[1;32m     37\u001b[0m train_loss \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mlogged_metrics[\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     38\u001b[0m result \u001b[39m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m: train_loss\n\u001b[1;32m     40\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1532\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1453\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1454\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1460\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   1461\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1532\u001b[0m     loaded \u001b[39m=\u001b[39m _load_from_checkpoint(\n\u001b[1;32m   1533\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1534\u001b[0m         checkpoint_path,\n\u001b[1;32m   1535\u001b[0m         map_location,\n\u001b[1;32m   1536\u001b[0m         hparams_file,\n\u001b[1;32m   1537\u001b[0m         strict,\n\u001b[1;32m   1538\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1539\u001b[0m     )\n\u001b[1;32m   1540\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/anaconda3/envs/parkinson/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:62\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     map_location \u001b[39m=\u001b[39m cast(_MAP_LOCATION_TYPE, \u001b[39mlambda\u001b[39;00m storage, loc: storage)\n\u001b[1;32m     61\u001b[0m \u001b[39mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 62\u001b[0m     checkpoint \u001b[39m=\u001b[39m pl_load(checkpoint_path, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m     64\u001b[0m \u001b[39m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     65\u001b[0m checkpoint \u001b[39m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     66\u001b[0m     checkpoint, checkpoint_path\u001b[39m=\u001b[39m(checkpoint_path \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(checkpoint_path, (\u001b[39mstr\u001b[39m, Path)) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/parkinson/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:50\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     46\u001b[0m         \u001b[39mstr\u001b[39m(path_or_url),\n\u001b[1;32m     47\u001b[0m         map_location\u001b[39m=\u001b[39mmap_location,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m fs \u001b[39m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 50\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39;49mopen(path_or_url, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mload(f, map_location\u001b[39m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m~/anaconda3/envs/parkinson/lib/python3.10/site-packages/fsspec/spec.py:1151\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     ac \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mautocommit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_intrans)\n\u001b[0;32m-> 1151\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(\n\u001b[1;32m   1152\u001b[0m         path,\n\u001b[1;32m   1153\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1154\u001b[0m         block_size\u001b[39m=\u001b[39;49mblock_size,\n\u001b[1;32m   1155\u001b[0m         autocommit\u001b[39m=\u001b[39;49mac,\n\u001b[1;32m   1156\u001b[0m         cache_options\u001b[39m=\u001b[39;49mcache_options,\n\u001b[1;32m   1157\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1158\u001b[0m     )\n\u001b[1;32m   1159\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1160\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mfsspec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/anaconda3/envs/parkinson/lib/python3.10/site-packages/fsspec/implementations/local.py:183\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_mkdir \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/parkinson/lib/python3.10/site-packages/fsspec/implementations/local.py:285\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression \u001b[39m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open()\n",
      "File \u001b[0;32m~/anaconda3/envs/parkinson/lib/python3.10/site-packages/fsspec/implementations/local.py:290\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mclosed:\n\u001b[1;32m    289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocommit \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode:\n\u001b[0;32m--> 290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    291\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression:\n\u001b[1;32m    292\u001b[0m             compress \u001b[39m=\u001b[39m compr[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression]\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/Users/taa/Desktop/parkinson/pfgp'"
     ]
    }
   ],
   "source": [
    "model = FOGTransformerEncoder(state=\"pretrain\")\n",
    "model, trainer, result = pretrain_model(PreTrainingFogModule, model, fog_train_loader, save_name=\"FOGTransformerEncoder\", optimizer_name=\"Adam\", optimizer_hparams={\"lr\": cfg.lr, \"weight_decay\": cfg.gamma})\n",
    "print(json.dumps(cfg.hparams, sort_keys=True, indent=4))\n",
    "print(json.dumps(result, sort_keys=True, indent=4))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,207,171 trainable parameters\n",
      "The model has 314,883 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(FOGTransformerEncoder()):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(FOGModel()):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train(model, optimizer, criterion, train_loader):\\n    print(\"Training...\")\\n    for x, y, _ in tqdm(train_loader):\\n        # print(y)\\n        print(x.shape, y.shape)\\n        #ic(x, y)\\n        # single forward pass\\n        # cast x to the correct data type\\n        x = x.float()\\n        y_hat = model(x)\\n        print(y_hat)\\n        # print(soft(y_hat))\\n        # print(y_hat.shape)\\n        # print(y_hat.argmax(dim=-1))\\n        # calculate loss\\n        loss = criterion(y_hat, y)\\n        print(loss.item())\\n        # calculate gradients\\n        loss.backward()\\n        # update weights\\n        optimizer.step()\\n        print(y)\\n        break\\nmodel = FOGTransformerEncoder(\"pre-train\")\\noptimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\\ncriterion = nn.MSELoss()\\nsoft = nn.Softmax(dim=-1)\\ntrain(model, optimizer, criterion, fog_train_loader)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def train(model, optimizer, criterion, train_loader):\n",
    "    print(\"Training...\")\n",
    "    for x, y, _ in tqdm(train_loader):\n",
    "        # print(y)\n",
    "        print(x.shape, y.shape)\n",
    "        #ic(x, y)\n",
    "        # single forward pass\n",
    "        # cast x to the correct data type\n",
    "        x = x.float()\n",
    "        y_hat = model(x)\n",
    "        print(y_hat)\n",
    "        # print(soft(y_hat))\n",
    "        # print(y_hat.shape)\n",
    "        # print(y_hat.argmax(dim=-1))\n",
    "        # calculate loss\n",
    "        loss = criterion(y_hat, y)\n",
    "        print(loss.item())\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        print(y)\n",
    "        break\n",
    "model = FOGTransformerEncoder(\"pre-train\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "criterion = nn.MSELoss()\n",
    "soft = nn.Softmax(dim=-1)\n",
    "train(model, optimizer, criterion, fog_train_loader)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parkinson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
